Using config:
{'B_VALIDATION': False,
 'CONFIG_NAME': 'attn2',
 'CUDA': True,
 'DATASET_NAME': 'faces',
 'DATA_DIR': '../data/face_image',
 'GAN': {'B_ATTENTION': True,
         'B_DCGAN': False,
         'CONDITION_DIM': 100,
         'DF_DIM': 64,
         'GF_DIM': 32,
         'R_NUM': 2,
         'Z_DIM': 100},
 'GPU_ID': 0,
 'RNN_TYPE': 'LSTM',
 'TEXT': {'CAPTIONS_PER_IMAGE': 4, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},
 'TRAIN': {'BATCH_SIZE': 22,
           'B_NET_D': True,
           'DISCRIMINATOR_LR': 0.001,
           'ENCODER_LR': 0.0002,
           'FLAG': True,
           'GENERATOR_LR': 0.001,
           'MAX_EPOCH': 100,
           'NET_E': '../DAMSMencoders/face_image/text_encoder70.pth',
           'NET_G': '',
           'RNN_GRAD_CLIP': 0.25,
           'SMOOTH': {'GAMMA1': 4.0,
                      'GAMMA2': 5.0,
                      'GAMMA3': 10.0,
                      'LAMBDA': 5.0},
           'SNAPSHOT_INTERVAL': 10},
 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},
 'WORKERS': 4}
Load filenames from: ../data/face_image/train/filenames_all.pickle (6001)
Load filenames from: ../data/face_image/test/filenames_all.pickle (1000)
Load from:  ../data/face_image/captions.pickle
Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth
Load image encoder from: ../DAMSMencoders/face_image/image_encoder70.pth
Traceback (most recent call last):
  File "main.py", line 143, in <module>
    algo.train()
  File "/userhome/34/ylpeng2/AttnGAN/code/trainer.py", line 222, in train
    text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()
  File "/userhome/34/ylpeng2/AttnGAN/code/trainer.py", line 70, in build_models
    text_encoder.load_state_dict(state_dict)
  File "/userhome/34/ylpeng2/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 847, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for RNN_ENCODER:
	size mismatch for encoder.weight: copying a param with shape torch.Size([41, 300]) from checkpoint, the shape in current model is torch.Size([27297, 300]).
